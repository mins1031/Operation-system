# Operation-system
**https://github.com/gyoogle/tech-interview-for-developer 참고 핵심 개념 이해 및 정리**
 ## 운영체제란?
 일반적으로 **하드웨어를 관리하고 응용프로그램과 하드웨어 사이에서 인터페이스 역할을 하며 시스템의 동작을 제어하는 시스템 소프트웨어** 로 정의한다. 즉 운영체제는 => **시스템의 자원과 동작을 관리하는 소프트웨어** 이다.
 
 운영체제를 큰 틀로 나눠보면 아래와 같다.
 1. 프로세스관리
 > 운영체제에서 작동하는 응용 프로그램을 관리하는 기능이다. 어떤 의미에서는 프로세서(CPU)를 관리하는 것이라고 볼수도 있다. 현재 CPU를 점유해야할 프로세스를 결정하고 실제로 CPU 메모리를 프로세스에 할당하며 프로세스간 공유자원 접근과 통신등을 관리한다.
   * 프로세스, 스레드
   * 스케줄링
   * 동기화
   * IPC통신
 2. 저장장치 관리
 > 1차 저장장치에 대항하는 메인 메모리와 2차 저장장치에 해당 하는 하드디스트,NAND등을 관리하는 기능
   * 1차 저장장치(Main memory)
     * 프로세스에 할당하는 메모리 영역의 할당과 해제
     * 각 메모리 영역간의 침범 방지
     * 메인 메모리의 효율적 활용을 위한 가상 메모리 기능
   * 2차 저장장치(HDD,NAND등) 
     * 파일형식의 데이터 저장
     * 이런 파일 데이터 관리를 위한 파일 시스템을 OS에서 관리
  
   * 메모리 관리
   * 가상 메모리
   * 파일 시스템
 3. 네트워킹
 > TCP/IP 기반 인터넷에 연결하거나 응용프로그램이 네트워크를 사용하려면 **운영 체제에서 네트워크 프로토콜을 지원해야 한다** 현재 상용 OS들은 다양하고 많은 네트워크 프로토콜을 지원하고 있다.
   * TCP/IP
   * 기타 프로토콜
 4. 사용지 관리
 > 하나의 PC는 한사람만 사용할 수도 있지만 여러사람이 사용할 수도 있다(서버처럼) 그래서 운영체제는 한 컴퓨터를 여러 사람이 사용하는 환경도 지원해야 한다. 따라서 운영체제는 각 계정을 관리할 수 있는 기능이 필요하고 각 계정별 개인 파일에 대해선 다른 사용자가 접근할 수 없도록 해야한다.
   * 계정 관리
   * 접근권한 관리
 5. 디바이스 드라이버
 > 운영체제는 시스템의 자원, 하드웨어를 관리한다. 시스템에는 여러 하드웨어가 붙어 있는데(마우스,키보드,스피커등) 이들을 운영체제에서 인식하고 관리하게 만들어 응용 프로그램이 하드웨어를 사용할 수 있게 만들어줘야한다. 따라서 운영체제 안에 하드웨어를 추상화해주는 인터페이스 계층이 필요하고 이 계층을 '디바이스 드라이버' 라고 한다. 하드웨어의 종류가 많은 만큼 운영체제 내의 디바이스 드라이버도 많이 존재한다. 이 디바이스 드라이버를 관리하는 기능 또한 운영체제가 하고 있다.
   * 순차접근 장치
   * 임의접근 장치
   * 네트워크 장치
 
 ## 프로세스와 스레드
 > **프로세스** : 프로그램을 메모리 상에서 실행중인 작업   
 > **스레드** : 프로세스 안에서 실행되는 여러 흐름 단위
 기본적으로 프로세스 마다 메인스레드를 포함해 최소 1개의 스레드를 가지고 있다
 * 프로그램이 CPU에 의해 실행되면 프로세스가 생성되고 메모리에 **프로세스 주소 공간**이 할당된다
 <img src="https://camo.githubusercontent.com/3dc4ad61f03160c310a855a4bd68a9f2a2c9a4c7/68747470733a2f2f74312e6461756d63646e2e6e65742f6366696c652f746973746f72792f393938383931343635433637433330363036"/>
 프로세스는 각각 별도의 주소공간을 할당 받는다(독립적)
 * Code : 코드 자체를 구성하는 메모리 영역.(=프로그램 소스 코드 저장)
 * Data : 전역변수, 정적변수, 배열등이 저장된다
 * Heap : 동적할당시 사용된다(new()연산등)
 * stack : 지역변수,매개변수, 리턴값,함수등이 저장된다.(임시 메모리 영역)
 
 * 스레드는 stack만 할당 받고 나머지 영역을 서로 공유한다.
 * 하나의 프로세스가 생성될 때 기본적으로 하나의 스레드가 같이 생성된다(메인 스레드)
 * Stack과 data를 나눈 이유는 스택 구조의 특성과 전역 변수의 활용성을 위해서 이다.
 * 프로세스는 자신만의 고유 메모리와 자원을 할당받아 사용하는데 스레드는 다른 스레드와 공간, 자원을 공유하면서 사용하는 차이가 존재한다.
 
 
  #### 멀티 프로세스
  > 하나의 컴퓨터에 여러 CPU가 장착되어 하나이상의 프로세스들을 동시에 병렬 처리한다.
  * 각 프로세스간 메모리 침범문제를 OS자체에서 해결할수 있어 안정적이나 각각 독립된 메모리 영역을 가지고 있음으로 작업량이 많을 수록 오버헤드가 발생하고 Context Switching으로 인한 성능 저하가 일어난다.(Context Switch 밑에 개시)
  
  #### 멀티 스레드
  > 하나의 응용 프로그램에서 여러 스레드를 구성해 각 스레드가 하나의 작업을 처리하는 것
  스레드들이 공유 메모리를 통해 다수의 작업을 동시에 처리하도록 해준다.
  
  장점: 독립적인 프로세스에 비해 공유메모리 만큼의 시간, 자원손실이 감소하고 전역변수와 정적변수에 대한 공유가 가능.
  단점 : 안전성 문제, 하나의 스레드가 데이터 공간을 망가뜨리면 모든 스레드가 작동 불능 상태가 된다. 이러한 단점은 "Critical Section기법"을 대비한다고 한다(추후 알아볼것.)
  
 ## 인터럽트
 > 프로그램을 실행하는 도중 예외나 에러가 발생할 경우 현재 실행중이 작업을 즉시 중단하고 발생된 상황에 대한 우선 처리가 필요함을 cpu에게 알리는것이다. 지금 수행중인 일보다 중요한 일이 발생하면 그일 먼저 처리하고 나서 하던일을 계속해야 한다.
 * 외/내부 인터럽트는 **CPU으 하드웨어 신호에 의해 발생된다.**
 * 외부 인터럽트
   * 입출력장치, 타이밍 장치, 전원 등 외부적인 요인으로 발생
   * 전원이상, 기계착오, 외부 신호, 입출력
 * 내부 인터럽트
   * Trap이라고 부르며, 잘못된 명령이나 데이터를 사용할 대 발생한다
   * 0으로 나누기가 발생, 스택오버플로우, 명령어를 잘못 사용한 경우(예외)  
 * 소프트웨어 인터럽트는 **명령어의 수행에 의해 발생된다.**
 * 소프트 웨어 인터럽트
   * 프로그램 처리중 명령의 요청에 의해 발생한것.
   * 대표적으로 예외 사항이 발생하거나 시스템콜이 실행되었을 때이다. 
  
 * 인터럽트를 발생시키기 위해 하드웨어/소프트웨어는 cpu내에 있는 `인터럽트 라인` 을 세팅하여 인터럽트를 발생시킨다. cpu는 매번 명령을 수행하기 전에 인터럽트 라인이 세팅되어있는지를 검사한다.
 
 #### 인터럽트 발생 처리 과정
 어떤 프로세스 a 실행중 디스크에서 어떤 데이터를 읽어오라는 명령을 받았다고 할때
   1) 프로세스 a는 시스템 콜을 통해 인터럽트를 발생시킨다
   2) CPU는 현재진행중인 기계어 코드를 완료한다
   3) 현재까지 수행중이었던 상태를 해당 프로세스의 PCB에 저장한다(메모리 주소, 레지스터값, 하드웨어 상태등...)
   4) 인터럽트 백터(인터럽트 발생시 처리해야할 인터럽트 핸들러의 주소를 인터럽트 별로 보관하고 있는 테이블이다.)를 읽고 ISR 주소값을 얻어 ISR(interrupt Service Routine)로 점프하여 루틴을 실행한다.
   5) 해당 일을 다 처리하면 PCB에 저장한 상태를 복원한다.
   6) ISR 끝 IRET명령어를 통해 인터럽트가 해제 된다
   7) 5의 내용을 복원하여 이전 실행 위치로 복귀한다.
 
 만약 인터럽트 기능이 없었으면 컨트롤러는 특정한 어떤일을 할 시기를 알기 위해 계속 체크해야 한다(이를 **폴링(Polling)이라고 한다.**) 폴링을 하는 시간엔 원래 하던 일에 집중할 수 없게 되어 많은 기능을 재대로 수행하지 못하는 단점이 있었다.

즉, 컨트롤러가 입력을 받아들이는 방법(우선순위 판별방법)에는 두가지가 있다.

  * 폴링 방식
    * 사용자가 명령어를 사용해 입력 핀의 값을 계속 읽어 변화를 알아내는 방식
    * 인터럽트 요청 플래그를 차례로 비교하여 우선순위가 가장 높은 인터럽트 자원을 찾아 이에 맞는 인터럽트 서비스 루틴을 수행한다. (하드웨어에 비해 속도 느림)
  * 인터럽트 방식
    * MCU 자체가 하드웨적으로 변화를 체크하여 변화 시에만 일정한 동작을 하는 방식
      * Daisy Chain
      * 병렬 우선순위 부여

인터럽트 방식은 하드웨어로 지원을 받아야 하는 제약이 있지만, 폴링에 비해 신속하게 대응하는 것이 가능하다. 따라서 **실시간 대응**이 필요할 때는 필수적인 기능이다.
즉, 인터럽트는 발생시기를 예측하기 힘든 경우에 컨트롤러가 가장 빠르게 대응할 수 있는 방법이다.
  
  #### 인터럽트와 특권 명령
  * 명령어의 종류
    * CPU가 수행하는 명령에는 `일반 명령`과 `특권 명령`이 있다
    * **일반명령**은 메모리에서 자료를 읽어오고, CPU에서 계산을 하는 등의 명령이고 모든 프로그램이 수행할 수 있는 명령이다. 
    * **특권명령**은 보안이 필요한 명령으로 입출력장치, 타이머등의 장치를 접근하는 명령이다. 특권명령은 항상 운영체제 만이 수행할 수 있다
  #### 커널모드 vs 유저 모드
  > 운영체제는 하드웨어적인 보안을 유지하기 위해 기본적으로 두가지 모드를 지원한다
  * 커널모드 : 커널모드는 운영체제가 CPU의 제어권을 가지고 명령을 수행하는 모드로 일반명령, 특권명령 모두 수행할 수 있다
  * 유저 모드 : 유저모드는 일반 사용자 프로그램이 CPU제어권을 가지고 명령을 수행하는 모드로 일반 명령만을 수행할 수 있다

 ## 시스템콜과 API
 > OS는 다양한 서비스들을 수행하기 위해 하드웨어를 직접적으로 관리한다. 반면, 응용프로그램은 OS가 제공하는 인터페이스를 통해서만 자원을 사용할 수 있다. OS가 제공하는 이러한 인터페이스를 시스템콜(System Call) 라고 한다.
이러한 시스템 콜은 직접 사용하기에 많은 어려움이 있다. 때문에 프로그래밍 언어들은 시스템콜을 편리하게 사용하기 위한 수단으로 API를 제공한다. API란 Application Programming Interface로 응용프로그램을 쉽게 만들기 위한 도구이다. 이러한 API와 시스템콜이 제공되기 때문에 많은 개발자들은 더욱 쉽게(?) 프로그래밍을 할 수 있게 되었다.
 * 위의 말은 응용프로그램으로 운영체제가 관리하는 모든 자원(네트워크, 디스트, 메모리 등등)을 프로세스가 필요로 하는 경우 **시스템 콜을 통해 사용해야 한다**는 것이다. 파일을 여는것,똑같은 프로세스 하나 더 만드는것 등등...
 <img src="https://i.imgur.com/YINBipo.png"/>
 * 보통 이런 시스템 콜들은 운영체제에서 사용하라고 만들어놓은 함수들로 구현되있으며 프로그래밍시엔 해당 헤더파일을 불러와 사용한다고 한다.
   ### 시스템 콜의 종류와 실행 예
   > 시스템 콜은 아래와 같이 크게 여섯가지로 분류된다.(UNIX 시스템에서의 시스템콜)
     1) 프로세스 제어
       * fork()
       * exit()
       * wait()
     2) 파일 조작
       * open()
       * read()
       * write()
       * close()
     3) 장치 조작
       * ioctl()
       * read()
       * write()
      4) 정보 유지
       * getpid()
       * alarm()
       * sleep()
      5) 통신
       * pipe()
       * shm_open()
       * mmap()
      6) 보호
       * chmod()
       * umask()
       * chown() 
 ```
 c언어로 간단하게 예를 들자면
 #include <stdio.h>
int main()
{
  ...
  printf("Hello World!");
  ...
  return 0;
}
 ```
 * print()함수의 실행은 유저모드에서 수행되어 stdio라이브러리를 호출하고 라이브러리는 시스템콜인 write()함수를 호출하고 실행의 흐름은 커널모드로 전환된다. 커널은 호출을 실행하여 모니터에 문자열을 출력하고 다시 유저모드로 돌아와 printf()함수의 다음 단계를 진행한다.
 
 <img src="https://i.imgur.com/UH0DDZt.png"/>
 
 ## PCB 와 Context Switching
 #### Process Management
 > CPU가 프로세스가 여러개일떄 CPU 스케줄링을 통해 관리하는 것을 말함
 이떄, CPU는 각 프로세스들이 누군지 알아야 관리가 가능하다.
 이렇게 프로세스들의 특징을 갖고 있는것이 `process Metadata`이다
   * Process Medadata
     * Process Id
     * Process State
     * Process Priority
     * CPU Registers
     * Owner
     * CPU Usage
     * Memory Usage
  이 메타데이터는 프로세스가 생성되면 PCB(Process control Block)이라는 곳에 저장된다.
  
  #### PCB(Process Control Block)
  > 프로세스 메타데이터들을 저장해 놓은 곳, 한 PCB안에는 한 프로세스의 정보가 담긴다.
  <img src="https://camo.githubusercontent.com/6341ca44629c93eb107563ab612ebfc581b2251ae312394b011ea70ee873b523/68747470733a2f2f74312e6461756d63646e2e6e65742f6366696c652f746973746f72792f323536373341353035384632313143323234"/>
  
  ```
  프로그램실행 -> 프로세스 생성 -> 프로세스 주소 공간 생성 -> 이 프로세스 메타데이터들이 PCB에 저장된다.
  ```
  * PCB는 운영체제가 프로세스를 제어하기 위해 정보를 저장해 놓는 곳으로, 프로세스의 사태정보를 저장하는 구조체이다.
  * 프로세스 상태관리와 문맥교환(Context Switching)을 위해 필요하다.
  * PCB는 프로세스 생성시 만들어지며 주기억장치에 유지된다.
  컴퓨터에 여러 프로세스가 돌아가고 있지만 컴퓨터는 이 많은 일(프로세스)을 동시에 처리하는 것이 아닌 TIME SHARING이라는 짧은 시간동안 왔다갔다 빠르게 처리해주는 것인데 이 방법이 효율적이고 빨라 동시에 돌아가는 것 처럼 느껴지는 것이다.
  위의 과정중 왔다갔다할 시 다른 프로세스를 불러와야 하는데 이전의 작업내용을 프로세스 단위로 정보를 저장해주는 PCB에 저장이 된다.
  * 그럼 Context Switching이란?
    * 실행중인 프로세스의 정보는 CPU내부의 `레지스터`에 저장하고 사용하고 있다.
    * 만약 실행중에 인터럽트가 콜된다면 지금 수행하는 프로세스를 잠시 내려놓고 다른 프로세스를 동작시켜야 한다
    * 그럼 현재 레지스터의 정보를 인터럽트 관련 프로세스의 정보로 대체되어야 하는데 이때 기존 레지스터의 값이 PCB에 저장이된다.(프로세스가 어디까지 수행되었는지 등등)
    * 이렇게 기존 프로세스(p1)는 PCB의 p1의 공간에 저장이 된다
    * **이렇게 수행중인 프로세스를 변경할때 레지스터에 프로세스의 정보가 바뀌는 것을 Context Switching이라고 한다.**
  * 주로 1) 인터럽트가 발생하거나  2) 실행중인 프로세스가 CPU사용 허가 시간을 모두 소모 하거나 3) I/O 입출력을 위해 대기해야 하는 경우(키보드로 부터 입력받는 동안 CPU가 놀면 안되니 I/O작업이 끝날때 까지 다른 프로세스 진행) Context Switching이 발생한다. 

 ## IPC 
 > 프로세스는 독립적으로 실행된다. 즉 다른 프로세스에게 영향을 받지 않는다고 할 수 있다(스레드는 프로세스 안에서 자원을 공유하므로 영향을 받는다). 이러한 독립적 구조를 가진 프로세스간의 통신을 해야하는 상황이 있을 것인데 이를 가능하게 해주는 것이 바로 **IPC통신** 이다
 프로세스는 커널이 제공하는 IPC설비를 이용해 프로세스간 통신을 할 수 있게 된다
 프로세스간 자원을 공유해서 사용해야 하는경우, 자원공유 뿐만 아니라 속도를 높이는 경우 등등 IPC통신이 필요하다
 * IPC통신은 메시지 교환과 데이터공유라는 대표적인 두가지 방법이 있다.
   1) 메세지 교환(Message Passing)
     * 커널이 memory protection을 위해 대리 전달해주는 것을 말한다.
     * 안전하고 동기화 문제가 없다(OS가 알아서 동기화 해준다.)
     * 하지만 성능이 떨어지는 단점을 가지고 있다.
     * 직접 커뮤니케이션과 간접 커뮤니테이션이 있다.
   2) 데이터 공유(shared memory)
     * 두 프로세스간의 공유된 메모리를 생성후 이용하는것
     * 성능이 좋지만 동기화 문제가 발생한다 
   <img src="https://media.vlpt.us/images/jieuni/post/7065c5c2-d4a0-43e9-80ba-39ba2285d081/image.png"/>
   위 사진과 함께보자면 
   * <메세지 교환>은 서로 자원을 협력하려면 운영체제의 도움을 받아야 한다. 프로세스 A가 커널로 메세지를 보내면 그걸 커널이 B에게 보내주는 것 이게 메시지 교환 방식이다.
     * 메세지 교환에서 `direct communication`은 커널에 메시지를 직접 주고 그걸 커널이 B에게 메시지를 직접 전달하고 
     * `indirect communication` 은 A가  '커널에 abc라는 메시지 박스를 넣어두고 너가 거기서 읽어가'하면 B가 ABC메시지 박스에 가서 읽어오는 방식이 있다.
     * 둘다 커널을 이용한 메세지 교환이지만 사진의 쌓여있는 메세지들을 보면 알 수 있듯이 번거롭고 오버헤드가 큰 것이 단점이다.
   * <데이터 공유(shared memory)>는 프로세스 A와 B가 모두 읽고 쓸수 있는 메모리를 만들고 거기서 주고 받고 할 수 있도록 하는 방식이고 그 메모리가 `shared memory`이다 
     * 당연히 성능적인 측면에선 shared memory가 좋지만 동기화 문제가 있다고 한다.   
  * 메세지 교환(Message Passing) 과 데이터 공유(shared memory) 이외의 방법들은 밑과 같다
    * 익명 Pipe
      * 파이프는 두개의 프로세스를 연결하는데 하나의 프로세스는 데이터를 쓰기만 하고 다른 하나는 데이터를 읽기만 할 수 있다.
      * 한쪽 방향으로만 통신이 가능한 반이중 통신 이라고도 부른다
      * 따라서 양쪽 송,수신 다 하고 싶으면 2개의 파이프를 만들어야 한다,
      * 매우 간단하게 사용할 수 있는 장점이 있고 단순한 데이터 흐름을 가질땐 파이프를 사용하는 것이 효율적이다. 
      * 단점은 전이중통신을 위해 2개를 만들어야 할 때는 구현이 복잡해지게 된다.
    * Named Pipe(FIFO)
      * 익명 파이프는 통신할 프로세스를 명확히 알 수 있는 경우 사용한다.
      * named파이프는 전혀 모르는 상태의 프로세스들 사이 통신에 사용한다
      * 즉 익명파이프의 확장된 형태로 다른 프로세스들과도 통신이 가능한 것이다.
      * 하지만 named파이프 역시 읽기/쓰기 동시에 불가능하기 때문에 익명파이프 처럼 전이중통신을 위해선 파이프를 2개 만들어야 한다.
    * 메모리 맵
      * 공유 메모리 처럼 메모리를 공유해준다. 
      * 메모리 맵은 열린 파일을 메모리에 맵핑 시켜서 공유하는 방식이다. 
      * 주로 파일로 대용량 데이터를 공유해야 할 때 사용한다
    * 소켓
      * 네트워크 소켓 통신을 통해 데이터를 공유한다.
      * 클라이언트와 서버가 소켓을 통해서 통신하는 구조로 원격에서 프로세스간 데이터를 공유할 때 사용한다.     

## CPU 스케줄링
> 프로세스가 구동하려면 다양한 시스템 자원이 필요하다.대표적으로 CPU와 입출력장치가 있는데 **최고의 성능을 내기 위해 자원을 어떤 프로세스에 얼마나 할당하는지 정책을 만드는 것을 CPU스케줄링** 이라고 한다.  
<img src="https://t1.daumcdn.net/cfile/tistory/27033450580366160E"/>

### 프로세스의 상태전이
  * 승인(Admitted): 프로세스 생성이 가능하여 승인됨
  * 스케줄러 디스패치(Scheduler Dispatch) : 준비 상태에 있는 프로세스 중 하나를 선택하여 실행시키는 것.
  * 인터럽트 : 예외, 입출력,이벤트 등이 발생하여 현재 실행중인 프로세스를 준비 상태로 바꾸고 해당 작업을 먼저 처리하는것.
  * 입출력 or 이벤트 대기 : 실행중인 프로세스가 입출력이나 이벤트를 처리해야 하는 경우 입출력/ 이벤트가 모두 끝날때 까지 대기 상태로 만드는 것이다
  * 입출력 or 이벤트 완료 : 입출력/이벤트 가 끝난 프로세스를 준비상태로 전환하여 스케줄러에 의해 선택될 수 있도록 만드는 것이다. 
### 선점/ 비선점 스케줄링
* CPU스케줄링은 크게 **선점스케줄링**과 **비선점 스케줄링** 두가지로 분류된다
* 선점스케줄링 
  * 프로세스가 CPU를 할당받아 실행중이더라도 운영체제가 CPU를 강제로 뺴앗을 수 있는 방식
  * CPU처리 시간이 매우긴 프로세스가 CPU사용 독점을 막을 수 있어 효율적이 운영이 가능하다
  * 잦은 문맥교환으로 오버헤드가 많이 발생한다
* 비선점 스케줄링
  * 프로세스가 CPU를 점유하고 있다면 이를 뺴앗을 수 없는 방식
  * 필요한 문맥교환만 일어나기 떄문에 오버헤드가 상대적으로 적지만 프로세스의 배치에 따라 효율성 차이가 많이 난다.
 ### CPU스케줄링 종류
 * 비선점 스케줄링
   1) FCFS(First Come First Served)
     * 선입선출 방식으로 준비 큐에 도착한 순서대로 CPU를 할당하는 비선점 방식이다.
     * 프로세스 CPU처리 시간을 따로 고려하지 않기때문에 매우 단순하고 공평한 방법이다
     * 다만 CPU처리 시간이 긴 프로세스가 먼저 올경우 CPU처리 시간이 짧은 프로세스도 처리에 오랜 시간이 걸릴수 있다는 점이 단점이다.
   2) SJF (Shortest job First)
     * 준비 큐에 있는 프로세스 중 실행 시간이 가장 짧은 작업 부터 CPU를 할당하는 비선점형 방식이다.
     * 늦게 도착해도 CPU처리시간이 앞의 프로세스보다 짧으면 먼저 CPU를 할당 받을 수 있다.
     * 다만 공평성에 어긋나고 처리시간이 긴 프로세스의 경우 짧은 프로세스가 계속해서 들어온다면 대기큐에서 CPU를 계속 할당받지 못할 수도 있다. 이를 starvation현상이라고 한다
   3) HRN (Highest Response Ratio Next)
     * SJF 스케줄링에 Aging 기법을 합친 비선점 방식이다.
     * Aging이란 나이를 먹는다는 의미 그대로 starvation현상을 해결하기 위해 대기 시간이 길어지면 우선순위를 높여주는 방법이다.
     * SJF의 보완 방식이지만 여전히 공평성이 말끔히 해결되진 않는다.
 * 선점 스케줄링
   1) Priority Scheduling (우선순위 스케줄링)
     * 정적/동적으로 우선순위를 부여하여 우선순위가 높은 순서대로 처리한다
     * 우선 순위가 낮은 프로세스가 무한정 기다리는 starvation현상이 생길수 있다
     * 여전히 starvation문제와 공평성 문제가 있다
   2) Round Robin
     * FCFS에 의해 프로세스들이 보내지면 각 프로세스는 동일한 시간의 `Time Quantum`만큼 CPU를 할당 받는다.
     * `Time Quantum` 과 `Time Slice`: 실행의 최소 단위 시간
     * 만약 Time Quantum(할당 시간)동안 처리를 다하지 못하면 CPU를 빼앗고 다음 프로세스에게 넘김으로 선점형 방식이고 빼앗긴 프로세스는 준비큐의 맨 뒤로 간다.
     * 모든 프로세스가 최초응답시간을 빠르게 보장받을 수 있다는 큰장점이 있다.
     * 다만 Time Quantum이 크면 FCFS와 같게 되고 작으면 문맥교환이 잦아셔 오버헤드가 증가한다.
   3) Multilevel Queue
   <img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLqYAc%2FbtqtW0lcMXw%2FkM2ex6H9lu5f2tSfjoRgvK%2Fimg.png"/>
   
   > 다단계 큐 스케줄링은 우선순위에 따라 주니 큐를 어러개 사용하는 방식이다. 당연히 우선순위가 높은 큐에 먼저 CPU가 할당 되어 큐에 속한 모든 프로세스가 처리되야 다음 우선순위 큐가 실행될 수 있다. 그리고 한번 순위가 매겨져 큐에 들어가면 우선순위는 바뀌지 않는다
   * 각 큐는 독립적인 스케줄링 알고리즘을 가질수 있는데 보통 전면 프로세스들이 속해있는 큐는 우선순위가 높고 라운드 로빈 스케줄링을 사용해 타임 슬라이스를 작게 한다.
   * 후면 프로세스에는 사용자와의 상호작용이 없으므로 가장 간단한 FCJS방식으로 처리한다. 보통 총 CPU시간이 전면 프로세스의 처리에 80, 후면 프로세스 처리에 20이 할당된다
   * 이 다단계 큐 알고리즘 역시 문제는 starvation현상과 공평성 문제이다.
   
   4) Multilevel feedback Queue
   * 다단계 큐의 공평성 문제를 완화하기 위해 신분하락이 가능한 알고리즘이다. 이 알고리즘에서는 우선순위가 변동되기 떄문에 큐 사이의 이동이 가능하다
   * 한 번 CPU를 할당받은 프로세스는 우선순위가 조금 낮아진다. 따라서 너 낮은 큐로 이동하게 된다.(우선순위가 높아져 상위큐로 갈수도 있다)
   * 그리고 우선순위가 높은 큐보다 우선순위가 낮은 큐에 타임 슬라이스 크기를 크게 주어 어렵게 얻은 CPU를 좀더 오랫동안 사용하게 해주기 위함이다.
   <img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbt1O9K%2FbtqtWHGcTpB%2F3OHZNOKR4XRoBV8njb5Gjk%2Fimg.png"/>




